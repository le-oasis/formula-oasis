{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Spark libraries\n",
    "from pyspark.sql import SparkSession\n",
    "# For Schema \n",
    "from pyspark.sql.types import *\n",
    "# For Column\n",
    "from pyspark.sql.functions import col\n",
    "# For Timestamp\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "# Creates a Column Object From a Literal Value \n",
    "from pyspark.sql.functions import lit\n",
    "# To Timestamp & Concatenate\n",
    "from pyspark.sql.functions import to_timestamp, concat \n",
    "# Distinct Counts\n",
    "from pyspark.sql.functions import countDistinct "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/oasis/sources/spark-3.2.0/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/oasis/sources/spark-3.2.0/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/oasis/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/oasis/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3acb5504-1a93-42f4-9d5d-2a4f20e78792;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.0.0 in central\n",
      "\tfound io.delta#delta-storage;2.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      ":: resolution report :: resolve 283ms :: artifacts dl 21ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3acb5504-1a93-42f4-9d5d-2a4f20e78792\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/8ms)\n",
      "23/10/10 13:29:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/10/10 13:29:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLake\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Lake Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_layer = \"s3a://oasiscorp-raw/formula-oasis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data and specify the schema in datareader API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/10 13:30:59 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    }
   ],
   "source": [
    "# Define the schema\n",
    "lap_times_schema = StructType(fields=[StructField(\"raceId\", IntegerType(), False),\n",
    "                                      StructField(\"driverId\", IntegerType(), True),\n",
    "                                      StructField(\"lap\", IntegerType(), True),\n",
    "                                      StructField(\"position\", IntegerType(), True),\n",
    "                                      StructField(\"time\", StringType(), True),\n",
    "                                      StructField(\"milliseconds\", IntegerType(), True)\n",
    "                                     ])\n",
    "\n",
    "# Read the data from the CSV file with the defined schema\n",
    "lap_times_sdf = spark.read \\\n",
    "                            .schema(lap_times_schema) \\\n",
    "                            .csv(f\"{raw_layer}/lap_times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raceId', 'driverId', 'lap', 'position', 'time', 'milliseconds']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lap_times_sdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+--------+--------+------------+\n",
      "|raceId|driverId|lap|position|    time|milliseconds|\n",
      "+------+--------+---+--------+--------+------------+\n",
      "|   841|      20|  1|       1|1:38.109|       98109|\n",
      "|   841|      20|  2|       1|1:33.006|       93006|\n",
      "|   841|      20|  3|       1|1:32.713|       92713|\n",
      "|   841|      20|  4|       1|1:32.803|       92803|\n",
      "|   841|      20|  5|       1|1:32.342|       92342|\n",
      "|   841|      20|  6|       1|1:32.605|       92605|\n",
      "|   841|      20|  7|       1|1:32.502|       92502|\n",
      "|   841|      20|  8|       1|1:32.537|       92537|\n",
      "|   841|      20|  9|       1|1:33.240|       93240|\n",
      "|   841|      20| 10|       1|1:32.572|       92572|\n",
      "|   841|      20| 11|       1|1:32.669|       92669|\n",
      "|   841|      20| 12|       1|1:32.902|       92902|\n",
      "|   841|      20| 13|       1|1:33.698|       93698|\n",
      "|   841|      20| 14|       3|1:52.075|      112075|\n",
      "|   841|      20| 15|       4|1:38.385|       98385|\n",
      "|   841|      20| 16|       2|1:31.548|       91548|\n",
      "|   841|      20| 17|       1|1:30.800|       90800|\n",
      "|   841|      20| 18|       1|1:31.810|       91810|\n",
      "|   841|      20| 19|       1|1:31.018|       91018|\n",
      "|   841|      20| 20|       1|1:31.055|       91055|\n",
      "+------+--------+---+--------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lap_times_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rename and add new dataframes\n",
    "\n",
    "laps_sdf_curated = lap_times_sdf \\\n",
    "    .withColumnRenamed(\"raceId\", \"race_id\") \\\n",
    "    .withColumnRenamed(\"driverId\", \"driver_id\") \\\n",
    "    .withColumn(\"ingested_date\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---+--------+--------+------------+--------------------+\n",
      "|race_id|driver_id|lap|position|    time|milliseconds|       ingested_date|\n",
      "+-------+---------+---+--------+--------+------------+--------------------+\n",
      "|    841|       20|  1|       1|1:38.109|       98109|2023-10-10 13:31:...|\n",
      "|    841|       20|  2|       1|1:33.006|       93006|2023-10-10 13:31:...|\n",
      "|    841|       20|  3|       1|1:32.713|       92713|2023-10-10 13:31:...|\n",
      "|    841|       20|  4|       1|1:32.803|       92803|2023-10-10 13:31:...|\n",
      "|    841|       20|  5|       1|1:32.342|       92342|2023-10-10 13:31:...|\n",
      "|    841|       20|  6|       1|1:32.605|       92605|2023-10-10 13:31:...|\n",
      "|    841|       20|  7|       1|1:32.502|       92502|2023-10-10 13:31:...|\n",
      "|    841|       20|  8|       1|1:32.537|       92537|2023-10-10 13:31:...|\n",
      "|    841|       20|  9|       1|1:33.240|       93240|2023-10-10 13:31:...|\n",
      "|    841|       20| 10|       1|1:32.572|       92572|2023-10-10 13:31:...|\n",
      "|    841|       20| 11|       1|1:32.669|       92669|2023-10-10 13:31:...|\n",
      "|    841|       20| 12|       1|1:32.902|       92902|2023-10-10 13:31:...|\n",
      "|    841|       20| 13|       1|1:33.698|       93698|2023-10-10 13:31:...|\n",
      "|    841|       20| 14|       3|1:52.075|      112075|2023-10-10 13:31:...|\n",
      "|    841|       20| 15|       4|1:38.385|       98385|2023-10-10 13:31:...|\n",
      "|    841|       20| 16|       2|1:31.548|       91548|2023-10-10 13:31:...|\n",
      "|    841|       20| 17|       1|1:30.800|       90800|2023-10-10 13:31:...|\n",
      "|    841|       20| 18|       1|1:31.810|       91810|2023-10-10 13:31:...|\n",
      "|    841|       20| 19|       1|1:31.018|       91018|2023-10-10 13:31:...|\n",
      "|    841|       20| 20|       1|1:31.055|       91055|2023-10-10 13:31:...|\n",
      "+-------+---------+---+--------+--------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "laps_sdf_curated.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Data to S3 in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_layer = \"s3a://oasiscorp-curated/formula-oasis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# write in parquet format\n",
    "\n",
    "laps_sdf_curated.write \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                    .parquet(f\"{processed_layer}/lap_times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data Back From S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---+--------+--------+------------+--------------------+\n",
      "|race_id|driver_id|lap|position|    time|milliseconds|       ingested_date|\n",
      "+-------+---------+---+--------+--------+------------+--------------------+\n",
      "|     67|       14| 26|      13|1:25.802|       85802|2023-10-10 13:32:...|\n",
      "|     67|       14| 27|      13|1:25.338|       85338|2023-10-10 13:32:...|\n",
      "|     67|       14| 28|      13|1:25.395|       85395|2023-10-10 13:32:...|\n",
      "|     67|       14| 29|      12|1:26.191|       86191|2023-10-10 13:32:...|\n",
      "|     67|       14| 30|      11|1:25.439|       85439|2023-10-10 13:32:...|\n",
      "|     67|       14| 31|      10|1:25.375|       85375|2023-10-10 13:32:...|\n",
      "|     67|       14| 32|      12|1:28.219|       88219|2023-10-10 13:32:...|\n",
      "|     67|       14| 33|      13|1:49.156|      109156|2023-10-10 13:32:...|\n",
      "|     67|       14| 34|      13|1:25.128|       85128|2023-10-10 13:32:...|\n",
      "|     67|       14| 35|      13|1:25.351|       85351|2023-10-10 13:32:...|\n",
      "+-------+---------+---+--------+--------+------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read From Parquet\n",
    "\n",
    "races_df = spark.read.parquet(f\"{processed_layer}/lap_times\")\n",
    "races_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oasis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

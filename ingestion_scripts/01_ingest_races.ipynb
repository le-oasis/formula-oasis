{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "import os\n",
    "import hashlib\n",
    "import urllib.request\n",
    "import json\n",
    "from datetime import timedelta, date\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/10 14:36:56 WARN Utils: Your hostname, OASIS-CORP.local resolves to a loopback address: 127.0.0.1; using 192.168.225.160 instead (on interface en0)\n",
      "23/10/10 14:36:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/oasis/sources/spark-3.2.0/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/oasis/sources/spark-3.2.0/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/oasis/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/oasis/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-626de8e2-d36b-48bb-9140-d99a5612330f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.0.0 in central\n",
      "\tfound io.delta#delta-storage;2.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      ":: resolution report :: resolve 228ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-626de8e2-d36b-48bb-9140-d99a5612330f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/7ms)\n",
      "23/10/10 14:36:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/10/10 14:36:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/10/10 14:36:58 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLake\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Lake Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_layer = \"s3a://oasiscorp-raw/formula-oasis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deifine Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "races_schema = StructType(fields=[StructField(\"raceID\", StringType(), False),\n",
    "                                  StructField(\"year\", StringType(), True),\n",
    "                                  StructField(\"round\", StringType(), True),\n",
    "                                    StructField(\"circuitId\", StringType(), True),\n",
    "                                    StructField(\"name\", StringType(), True),\n",
    "                                    StructField(\"date\", StringType(), True),\n",
    "                                    StructField(\"time\", StringType(), True),\n",
    "                                    StructField(\"url\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data and specify the schema in datareader API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/10 14:37:01 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    }
   ],
   "source": [
    "# Read the data from the raw layer\n",
    "races_sdf = spark.read \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .schema(races_schema) \\\n",
    "                .csv(f\"{raw_layer}/races.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+---------+--------------------+----------+--------+--------------------+\n",
      "|raceID|year|round|circuitId|                name|      date|    time|                 url|\n",
      "+------+----+-----+---------+--------------------+----------+--------+--------------------+\n",
      "|     1|2009|    1|        1|Australian Grand ...|2009-03-29|06:00:00|http://en.wikiped...|\n",
      "|     2|2009|    2|        2|Malaysian Grand Prix|2009-04-05|09:00:00|http://en.wikiped...|\n",
      "|     3|2009|    3|       17|  Chinese Grand Prix|2009-04-19|07:00:00|http://en.wikiped...|\n",
      "|     4|2009|    4|        3|  Bahrain Grand Prix|2009-04-26|12:00:00|http://en.wikiped...|\n",
      "|     5|2009|    5|        4|  Spanish Grand Prix|2009-05-10|12:00:00|http://en.wikiped...|\n",
      "|     6|2009|    6|        6|   Monaco Grand Prix|2009-05-24|12:00:00|http://en.wikiped...|\n",
      "|     7|2009|    7|        5|  Turkish Grand Prix|2009-06-07|12:00:00|http://en.wikiped...|\n",
      "|     8|2009|    8|        9|  British Grand Prix|2009-06-21|12:00:00|http://en.wikiped...|\n",
      "|     9|2009|    9|       20|   German Grand Prix|2009-07-12|12:00:00|http://en.wikiped...|\n",
      "|    10|2009|   10|       11|Hungarian Grand Prix|2009-07-26|12:00:00|http://en.wikiped...|\n",
      "+------+----+-----+---------+--------------------+----------+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "races_sdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raceID', 'year', 'round', 'circuitId', 'name', 'date', 'time', 'url']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "races_sdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+---------+--------------------+----------+--------+--------------------+--------------------+-------------------+\n",
      "|raceID|year|round|circuitId|                name|      date|    time|                 url| ingestion_timestamp|     race_timestamp|\n",
      "+------+----+-----+---------+--------------------+----------+--------+--------------------+--------------------+-------------------+\n",
      "|     1|2009|    1|        1|Australian Grand ...|2009-03-29|06:00:00|http://en.wikiped...|2023-10-10 14:37:...|2009-03-29 06:00:00|\n",
      "|     2|2009|    2|        2|Malaysian Grand Prix|2009-04-05|09:00:00|http://en.wikiped...|2023-10-10 14:37:...|2009-04-05 09:00:00|\n",
      "|     3|2009|    3|       17|  Chinese Grand Prix|2009-04-19|07:00:00|http://en.wikiped...|2023-10-10 14:37:...|2009-04-19 07:00:00|\n",
      "|     4|2009|    4|        3|  Bahrain Grand Prix|2009-04-26|12:00:00|http://en.wikiped...|2023-10-10 14:37:...|2009-04-26 12:00:00|\n",
      "|     5|2009|    5|        4|  Spanish Grand Prix|2009-05-10|12:00:00|http://en.wikiped...|2023-10-10 14:37:...|2009-05-10 12:00:00|\n",
      "|     6|2009|    6|        6|   Monaco Grand Prix|2009-05-24|12:00:00|http://en.wikiped...|2023-10-10 14:37:...|2009-05-24 12:00:00|\n",
      "|     7|2009|    7|        5|  Turkish Grand Prix|2009-06-07|12:00:00|http://en.wikiped...|2023-10-10 14:37:...|2009-06-07 12:00:00|\n",
      "|     8|2009|    8|        9|  British Grand Prix|2009-06-21|12:00:00|http://en.wikiped...|2023-10-10 14:37:...|2009-06-21 12:00:00|\n",
      "|     9|2009|    9|       20|   German Grand Prix|2009-07-12|12:00:00|http://en.wikiped...|2023-10-10 14:37:...|2009-07-12 12:00:00|\n",
      "|    10|2009|   10|       11|Hungarian Grand Prix|2009-07-26|12:00:00|http://en.wikiped...|2023-10-10 14:37:...|2009-07-26 12:00:00|\n",
      "+------+----+-----+---------+--------------------+----------+--------+--------------------+--------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame named 'races_sdf'\n",
    "races_sdf_curated = races_sdf.withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "                                .withColumn(\"race_timestamp\", to_timestamp(concat(col(\"date\"), lit(\" \"), col(\"time\")), 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "races_sdf_curated.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+-----+--------------------+--------------------+-------------------+\n",
      "|race_id|race_year|circuit_id|round|                name|       ingested_date|     race_timestamp|\n",
      "+-------+---------+----------+-----+--------------------+--------------------+-------------------+\n",
      "|      1|     2009|         1|    1|Australian Grand ...|2023-10-10 14:37:...|2009-03-29 06:00:00|\n",
      "|      2|     2009|         2|    2|Malaysian Grand Prix|2023-10-10 14:37:...|2009-04-05 09:00:00|\n",
      "|      3|     2009|        17|    3|  Chinese Grand Prix|2023-10-10 14:37:...|2009-04-19 07:00:00|\n",
      "|      4|     2009|         3|    4|  Bahrain Grand Prix|2023-10-10 14:37:...|2009-04-26 12:00:00|\n",
      "|      5|     2009|         4|    5|  Spanish Grand Prix|2023-10-10 14:37:...|2009-05-10 12:00:00|\n",
      "|      6|     2009|         6|    6|   Monaco Grand Prix|2023-10-10 14:37:...|2009-05-24 12:00:00|\n",
      "|      7|     2009|         5|    7|  Turkish Grand Prix|2023-10-10 14:37:...|2009-06-07 12:00:00|\n",
      "|      8|     2009|         9|    8|  British Grand Prix|2023-10-10 14:37:...|2009-06-21 12:00:00|\n",
      "|      9|     2009|        20|    9|   German Grand Prix|2023-10-10 14:37:...|2009-07-12 12:00:00|\n",
      "|     10|     2009|        11|   10|Hungarian Grand Prix|2023-10-10 14:37:...|2009-07-26 12:00:00|\n",
      "+-------+---------+----------+-----+--------------------+--------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "races_sdf_select = races_sdf_curated.select(\n",
    "    col(\"raceID\").alias(\"race_id\"),\n",
    "    col(\"year\").alias(\"race_year\"),\n",
    "    col(\"circuitId\").alias(\"circuit_id\"),\n",
    "    col(\"round\"),\n",
    "    col(\"name\"),\n",
    "    col(\"ingestion_timestamp\").alias(\"ingested_date\"),\n",
    "    col(\"race_timestamp\")\n",
    ")\n",
    "\n",
    "races_sdf_select.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- race_id: string (nullable = true)\n",
      " |-- race_year: string (nullable = true)\n",
      " |-- circuit_id: string (nullable = true)\n",
      " |-- round: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- ingested_date: timestamp (nullable = false)\n",
      " |-- race_timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print Schema \n",
    "\n",
    "races_sdf_select.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Data to S3 in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_layer = \"s3a://oasiscorp-curated/formula-oasis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "races_sdf_select.write \\\n",
    "                    .mode(\"overwrite\") \\\n",
    "                        .partitionBy(\"race_year\") \\\n",
    "                            .parquet(f\"{processed_layer}/races\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data Back From S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+--------------------+--------------------+-------------------+---------+\n",
      "|race_id|circuit_id|round|                name|       ingested_date|     race_timestamp|race_year|\n",
      "+-------+----------+-----+--------------------+--------------------+-------------------+---------+\n",
      "|   1053|        21|    2|Emilia Romagna Gr...|2023-10-10 14:38:...|2021-04-18 13:00:00|     2021|\n",
      "|   1052|         3|    1|  Bahrain Grand Prix|2023-10-10 14:38:...|2021-03-28 15:00:00|     2021|\n",
      "|   1051|         1|   21|Australian Grand ...|2023-10-10 14:38:...|2021-11-21 06:00:00|     2021|\n",
      "|   1054|        20|    3|                 TBC|2023-10-10 14:38:...|               null|     2021|\n",
      "|   1055|         4|    4|  Spanish Grand Prix|2023-10-10 14:38:...|2021-05-09 13:00:00|     2021|\n",
      "|   1056|         6|    5|   Monaco Grand Prix|2023-10-10 14:38:...|2021-05-23 13:00:00|     2021|\n",
      "|   1057|        73|    6|Azerbaijan Grand ...|2023-10-10 14:38:...|2021-06-06 12:00:00|     2021|\n",
      "|   1058|         7|    7| Canadian Grand Prix|2023-10-10 14:38:...|2021-06-13 18:00:00|     2021|\n",
      "|   1059|        34|    8|   French Grand Prix|2023-10-10 14:38:...|2021-06-27 13:00:00|     2021|\n",
      "|   1060|        70|    9| Austrian Grand Prix|2023-10-10 14:38:...|2021-07-04 13:00:00|     2021|\n",
      "+-------+----------+-----+--------------------+--------------------+-------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read From Parquet\n",
    "\n",
    "races_df = spark.read.parquet(f\"{processed_layer}/races\")\n",
    "races_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:===============================================>         (10 + 2) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|race_year|\n",
      "+---------+\n",
      "|     2018|\n",
      "|     2019|\n",
      "|     2012|\n",
      "|     2016|\n",
      "|     2017|\n",
      "|     2021|\n",
      "|     2015|\n",
      "|     2013|\n",
      "|     2014|\n",
      "|     2020|\n",
      "|     2010|\n",
      "|     2011|\n",
      "|     2007|\n",
      "|     2006|\n",
      "|     2004|\n",
      "|     2009|\n",
      "|     2005|\n",
      "|     2008|\n",
      "|     1977|\n",
      "|     1978|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# grab distinct race years \n",
    "races_df.select(\"race_year\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oasis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

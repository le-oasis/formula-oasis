{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "import os\n",
    "import hashlib\n",
    "import urllib.request\n",
    "import json\n",
    "from datetime import timedelta, date\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/oasis/sources/spark-3.2.0/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/oasis/sources/spark-3.2.0/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/oasis/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/oasis/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d23d120a-b5fb-40ce-a7f6-e4e2782e4f04;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.0.0 in central\n",
      "\tfound io.delta#delta-storage;2.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      ":: resolution report :: resolve 270ms :: artifacts dl 9ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d23d120a-b5fb-40ce-a7f6-e4e2782e4f04\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/6ms)\n",
      "23/10/10 13:24:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/10/10 13:24:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/10/10 13:24:29 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/10/10 13:24:29 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/10/10 13:24:29 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "23/10/10 13:24:29 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLake\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Lake Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_layer = \"s3a://oasiscorp-raw/formula-oasis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data and specify the schema in datareader API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/10 13:24:32 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    }
   ],
   "source": [
    "# Define the schema\n",
    "# Define the schema using StructType and StructField\n",
    "pit_schema = StructType([\n",
    "\n",
    "    StructField(\"raceId\", IntegerType(), nullable=True),\n",
    "    StructField(\"driverId\", IntegerType(), nullable=True),\n",
    "    StructField(\"stop\", StringType(), nullable=True),\n",
    "    StructField(\"lap\", IntegerType(), nullable=False),\n",
    "    StructField(\"time\", StringType(), nullable=True),\n",
    "    StructField(\"duration\", StringType(), nullable=True),\n",
    "    StructField(\"milliseconds\", IntegerType(), nullable=True)   \n",
    "])\n",
    "\n",
    "\n",
    "pits_sdf = spark.read \\\n",
    "                            .schema(pit_schema) \\\n",
    "                            .option(\"multiline\", True) \\\n",
    "                            .json(f\"{raw_layer}/pit_stops.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raceId', 'driverId', 'stop', 'lap', 'time', 'duration', 'milliseconds']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pits_sdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+---+--------+--------+------------+\n",
      "|raceId|driverId|stop|lap|    time|duration|milliseconds|\n",
      "+------+--------+----+---+--------+--------+------------+\n",
      "|   841|     153|   1|  1|17:05:23|  26.898|       26898|\n",
      "|   841|      30|   1|  1|17:05:52|  25.021|       25021|\n",
      "|   841|      17|   1| 11|17:20:48|  23.426|       23426|\n",
      "|   841|       4|   1| 12|17:22:34|  23.251|       23251|\n",
      "|   841|      13|   1| 13|17:24:10|  23.842|       23842|\n",
      "|   841|      22|   1| 13|17:24:29|  23.643|       23643|\n",
      "|   841|      20|   1| 14|17:25:17|  22.603|       22603|\n",
      "|   841|     814|   1| 14|17:26:03|  24.863|       24863|\n",
      "|   841|     816|   1| 14|17:26:50|  25.259|       25259|\n",
      "|   841|      67|   1| 15|17:27:34|  25.342|       25342|\n",
      "|   841|       2|   1| 15|17:27:41|  22.994|       22994|\n",
      "|   841|       1|   1| 16|17:28:24|  23.227|       23227|\n",
      "|   841|     808|   1| 16|17:28:39|  24.535|       24535|\n",
      "|   841|       3|   1| 16|17:29:00|  23.716|       23716|\n",
      "|   841|     155|   1| 16|17:29:06|  24.064|       24064|\n",
      "|   841|      16|   1| 16|17:29:08|  25.978|       25978|\n",
      "|   841|      15|   1| 16|17:29:49|  24.899|       24899|\n",
      "|   841|      18|   1| 17|17:30:24|  16.867|       16867|\n",
      "|   841|     153|   2| 17|17:31:06|  24.463|       24463|\n",
      "|   841|       5|   1| 17|17:31:11|  24.865|       24865|\n",
      "+------+--------+----+---+--------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pits_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rename and add new dataframes\n",
    "\n",
    "pits_sdf_curated = pits_sdf \\\n",
    "        .withColumnRenamed(\"raceId\", \"race_id\") \\\n",
    "        .withColumnRenamed(\"driverId\", \"driver_id\") \\\n",
    "        .withColumn(\"ingested_date\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----+---+--------+--------+------------+--------------------+\n",
      "|race_id|driver_id|stop|lap|    time|duration|milliseconds|       ingested_date|\n",
      "+-------+---------+----+---+--------+--------+------------+--------------------+\n",
      "|    841|      153|   1|  1|17:05:23|  26.898|       26898|2023-10-10 13:25:...|\n",
      "|    841|       30|   1|  1|17:05:52|  25.021|       25021|2023-10-10 13:25:...|\n",
      "|    841|       17|   1| 11|17:20:48|  23.426|       23426|2023-10-10 13:25:...|\n",
      "|    841|        4|   1| 12|17:22:34|  23.251|       23251|2023-10-10 13:25:...|\n",
      "|    841|       13|   1| 13|17:24:10|  23.842|       23842|2023-10-10 13:25:...|\n",
      "|    841|       22|   1| 13|17:24:29|  23.643|       23643|2023-10-10 13:25:...|\n",
      "|    841|       20|   1| 14|17:25:17|  22.603|       22603|2023-10-10 13:25:...|\n",
      "|    841|      814|   1| 14|17:26:03|  24.863|       24863|2023-10-10 13:25:...|\n",
      "|    841|      816|   1| 14|17:26:50|  25.259|       25259|2023-10-10 13:25:...|\n",
      "|    841|       67|   1| 15|17:27:34|  25.342|       25342|2023-10-10 13:25:...|\n",
      "|    841|        2|   1| 15|17:27:41|  22.994|       22994|2023-10-10 13:25:...|\n",
      "|    841|        1|   1| 16|17:28:24|  23.227|       23227|2023-10-10 13:25:...|\n",
      "|    841|      808|   1| 16|17:28:39|  24.535|       24535|2023-10-10 13:25:...|\n",
      "|    841|        3|   1| 16|17:29:00|  23.716|       23716|2023-10-10 13:25:...|\n",
      "|    841|      155|   1| 16|17:29:06|  24.064|       24064|2023-10-10 13:25:...|\n",
      "|    841|       16|   1| 16|17:29:08|  25.978|       25978|2023-10-10 13:25:...|\n",
      "|    841|       15|   1| 16|17:29:49|  24.899|       24899|2023-10-10 13:25:...|\n",
      "|    841|       18|   1| 17|17:30:24|  16.867|       16867|2023-10-10 13:25:...|\n",
      "|    841|      153|   2| 17|17:31:06|  24.463|       24463|2023-10-10 13:25:...|\n",
      "|    841|        5|   1| 17|17:31:11|  24.865|       24865|2023-10-10 13:25:...|\n",
      "+-------+---------+----+---+--------+--------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pits_sdf_curated.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Data to S3 in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_layer = \"s3a://oasiscorp-curated/formula-oasis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# write in parquet format\n",
    "\n",
    "pits_sdf_curated.write \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                    .parquet(f\"{processed_layer}/pit_stops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data Back From S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----+---+--------+--------+------------+--------------------+\n",
      "|race_id|driver_id|stop|lap|    time|duration|milliseconds|       ingested_date|\n",
      "+-------+---------+----+---+--------+--------+------------+--------------------+\n",
      "|    841|      153|   1|  1|17:05:23|  26.898|       26898|2023-10-10 13:25:...|\n",
      "|    841|       30|   1|  1|17:05:52|  25.021|       25021|2023-10-10 13:25:...|\n",
      "|    841|       17|   1| 11|17:20:48|  23.426|       23426|2023-10-10 13:25:...|\n",
      "|    841|        4|   1| 12|17:22:34|  23.251|       23251|2023-10-10 13:25:...|\n",
      "|    841|       13|   1| 13|17:24:10|  23.842|       23842|2023-10-10 13:25:...|\n",
      "|    841|       22|   1| 13|17:24:29|  23.643|       23643|2023-10-10 13:25:...|\n",
      "|    841|       20|   1| 14|17:25:17|  22.603|       22603|2023-10-10 13:25:...|\n",
      "|    841|      814|   1| 14|17:26:03|  24.863|       24863|2023-10-10 13:25:...|\n",
      "|    841|      816|   1| 14|17:26:50|  25.259|       25259|2023-10-10 13:25:...|\n",
      "|    841|       67|   1| 15|17:27:34|  25.342|       25342|2023-10-10 13:25:...|\n",
      "+-------+---------+----+---+--------+--------+------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read From Parquet\n",
    "\n",
    "races_df = spark.read.parquet(f\"{processed_layer}/pit_stops\")\n",
    "races_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oasis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Spark libraries\n",
    "from pyspark.sql import SparkSession\n",
    "# For Schema \n",
    "from pyspark.sql.types import *\n",
    "# For Column\n",
    "from pyspark.sql.functions import col\n",
    "# For Timestamp\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "# Creates a Column Object From a Literal Value \n",
    "from pyspark.sql.functions import lit\n",
    "# To Timestamp & Concatenate\n",
    "from pyspark.sql.functions import to_timestamp, concat \n",
    "# Distinct Counts\n",
    "from pyspark.sql.functions import countDistinct "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/oasis/sources/spark-3.2.0/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/oasis/sources/spark-3.2.0/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/oasis/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/oasis/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c7324cd6-4c38-4963-abe0-08bdb05ebdb3;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.0.0 in central\n",
      "\tfound io.delta#delta-storage;2.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      ":: resolution report :: resolve 248ms :: artifacts dl 11ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c7324cd6-4c38-4963-abe0-08bdb05ebdb3\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/6ms)\n",
      "23/10/10 13:37:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/10/10 13:37:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/10/10 13:37:55 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLake\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Lake Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_layer = \"s3a://oasiscorp-raw/formula-oasis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data and specify the schema in datareader API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/10 13:42:33 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    }
   ],
   "source": [
    "# Define the schema\n",
    "qualifying_schema = StructType(fields=[StructField(\"qualifyId\", IntegerType(), False),\n",
    "                                      StructField(\"raceId\", IntegerType(), True),\n",
    "                                      StructField(\"driverId\", IntegerType(), True),\n",
    "                                      StructField(\"constructorId\", IntegerType(), True),\n",
    "                                      StructField(\"number\", IntegerType(), True),\n",
    "                                      StructField(\"position\", IntegerType(), True),\n",
    "                                      StructField(\"q1\", StringType(), True),\n",
    "                                      StructField(\"q2\", StringType(), True),\n",
    "                                      StructField(\"q3\", StringType(), True),\n",
    "                                     ])\n",
    "\n",
    "# Read the data from the CSV file with the defined schema\n",
    "qual_sdf = spark.read \\\n",
    "                            .schema(qualifying_schema) \\\n",
    "                            .option(\"multiLine\", True) \\\n",
    "                            .json(f\"{raw_layer}/qualifying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qualifyId',\n",
       " 'raceId',\n",
       " 'driverId',\n",
       " 'constructorId',\n",
       " 'number',\n",
       " 'position',\n",
       " 'q1',\n",
       " 'q2',\n",
       " 'q3']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qual_sdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename & Add new dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qual_sdf_curated = qual_sdf.withColumnRenamed(\"qualifyId\", \"qualify_id\") \\\n",
    ".withColumnRenamed(\"driverId\", \"driver_id\") \\\n",
    ".withColumnRenamed(\"raceId\", \"race_id\") \\\n",
    ".withColumnRenamed(\"constructorId\", \"constructor_id\") \\\n",
    ".withColumn(\"ingestion_date\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------+--------------+------+--------+--------+--------+--------+--------------------+\n",
      "|qualify_id|race_id|driver_id|constructor_id|number|position|      q1|      q2|      q3|      ingestion_date|\n",
      "+----------+-------+---------+--------------+------+--------+--------+--------+--------+--------------------+\n",
      "|         1|     18|        1|             1|    22|       1|1:26.572|1:25.187|1:26.714|2023-10-10 13:44:...|\n",
      "|         2|     18|        9|             2|     4|       2|1:26.103|1:25.315|1:26.869|2023-10-10 13:44:...|\n",
      "|         3|     18|        5|             1|    23|       3|1:25.664|1:25.452|1:27.079|2023-10-10 13:44:...|\n",
      "|         4|     18|       13|             6|     2|       4|1:25.994|1:25.691|1:27.178|2023-10-10 13:44:...|\n",
      "|         5|     18|        2|             2|     3|       5|1:25.960|1:25.518|1:27.236|2023-10-10 13:44:...|\n",
      "|         6|     18|       15|             7|    11|       6|1:26.427|1:26.101|1:28.527|2023-10-10 13:44:...|\n",
      "|         7|     18|        3|             3|     7|       7|1:26.295|1:26.059|1:28.687|2023-10-10 13:44:...|\n",
      "|         8|     18|       14|             9|     9|       8|1:26.381|1:26.063|1:29.041|2023-10-10 13:44:...|\n",
      "|         9|     18|       10|             7|    12|       9|1:26.919|1:26.164|1:29.593|2023-10-10 13:44:...|\n",
      "|        10|     18|       20|             5|    15|      10|1:26.702|1:25.842|      \\N|2023-10-10 13:44:...|\n",
      "|        11|     18|       22|            11|    17|      11|1:26.369|1:26.173|      \\N|2023-10-10 13:44:...|\n",
      "|        12|     18|        4|             4|     5|      12|1:26.907|1:26.188|      \\N|2023-10-10 13:44:...|\n",
      "|        13|     18|       18|            11|    16|      13|1:26.712|1:26.259|      \\N|2023-10-10 13:44:...|\n",
      "|        14|     18|        6|             3|     8|      14|1:26.891|1:26.413|      \\N|2023-10-10 13:44:...|\n",
      "|        15|     18|       17|             9|    10|      15|1:26.914|      \\N|      \\N|2023-10-10 13:44:...|\n",
      "|        16|     18|        8|             6|     1|      16|1:26.140|      \\N|      \\N|2023-10-10 13:44:...|\n",
      "|        17|     18|       21|            10|    21|      17|1:27.207|      \\N|      \\N|2023-10-10 13:44:...|\n",
      "|        18|     18|        7|             5|    14|      18|1:27.446|      \\N|      \\N|2023-10-10 13:44:...|\n",
      "|        19|     18|       16|            10|    20|      19|1:27.859|      \\N|      \\N|2023-10-10 13:44:...|\n",
      "|        20|     18|       11|             8|    18|      20|1:28.208|      \\N|      \\N|2023-10-10 13:44:...|\n",
      "+----------+-------+---------+--------------+------+--------+--------+--------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "qual_sdf_curated.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Data to S3 in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_layer = \"s3a://oasiscorp-curated/formula-oasis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# write in parquet format\n",
    "\n",
    "qual_sdf_curated.write \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                    .parquet(f\"{processed_layer}/qualifying\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data Back From S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------+--------------+------+--------+--------+--------+--------+--------------------+\n",
      "|qualify_id|race_id|driver_id|constructor_id|number|position|      q1|      q2|      q3|      ingestion_date|\n",
      "+----------+-------+---------+--------------+------+--------+--------+--------+--------+--------------------+\n",
      "|         1|     18|        1|             1|    22|       1|1:26.572|1:25.187|1:26.714|2023-10-10 13:45:...|\n",
      "|         2|     18|        9|             2|     4|       2|1:26.103|1:25.315|1:26.869|2023-10-10 13:45:...|\n",
      "|         3|     18|        5|             1|    23|       3|1:25.664|1:25.452|1:27.079|2023-10-10 13:45:...|\n",
      "|         4|     18|       13|             6|     2|       4|1:25.994|1:25.691|1:27.178|2023-10-10 13:45:...|\n",
      "|         5|     18|        2|             2|     3|       5|1:25.960|1:25.518|1:27.236|2023-10-10 13:45:...|\n",
      "|         6|     18|       15|             7|    11|       6|1:26.427|1:26.101|1:28.527|2023-10-10 13:45:...|\n",
      "|         7|     18|        3|             3|     7|       7|1:26.295|1:26.059|1:28.687|2023-10-10 13:45:...|\n",
      "|         8|     18|       14|             9|     9|       8|1:26.381|1:26.063|1:29.041|2023-10-10 13:45:...|\n",
      "|         9|     18|       10|             7|    12|       9|1:26.919|1:26.164|1:29.593|2023-10-10 13:45:...|\n",
      "|        10|     18|       20|             5|    15|      10|1:26.702|1:25.842|      \\N|2023-10-10 13:45:...|\n",
      "+----------+-------+---------+--------------+------+--------+--------+--------+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read From Parquet\n",
    "\n",
    "races_df = spark.read.parquet(f\"{processed_layer}/qualifying\")\n",
    "races_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oasis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
